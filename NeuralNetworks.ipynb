{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqfT16J+AIeXIt0Bry6kt7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashishpathak78/ML_-Practice/blob/main/NeuralNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    # Activation function: f(x) = 1 / (1 + e^(-x))\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class Neuron:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def feedforward(self, inputs):\n",
        "        # Weighted sum + bias, then apply activation\n",
        "        total = np.dot(self.weights, inputs) + self.bias\n",
        "        return sigmoid(total)\n",
        "\n",
        "weights = np.array([0, 1])\n",
        "bias = 4\n",
        "n = Neuron(weights, bias)\n",
        "\n",
        "x = np.array([2, 3])\n",
        "print(n.feedforward(x))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jO4m-souMxX7",
        "outputId": "92aeffae-17c6-4595-aec7-6ee516514714"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9990889488055994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xYnTaboUD4XI",
        "outputId": "d4cd6202-1fa2-4462-bd19-92802e97632e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions for AND Gate:\n",
            "Input: [0 0] -> Output: 0\n",
            "Input: [0 1] -> Output: 0\n",
            "Input: [1 0] -> Output: 0\n",
            "Input: [1 1] -> Output: 1\n",
            "----------------------------------------\n",
            "Predictions for OR Gate:\n",
            "Input: [0 0] -> Output: 0\n",
            "Input: [0 1] -> Output: 1\n",
            "Input: [1 0] -> Output: 1\n",
            "Input: [1 1] -> Output: 1\n",
            "----------------------------------------\n",
            "Predictions for XOR Gate:\n",
            "Input: [0 0] -> Output: 1\n",
            "Input: [0 1] -> Output: 1\n",
            "Input: [1 0] -> Output: 0\n",
            "Input: [1 1] -> Output: 0\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.1, epochs=100):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def step_function(self, x):\n",
        "        return np.where(x >= 0, 1, 0)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        # Initialize weights and bias\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Training process\n",
        "        for _ in range(self.epochs):\n",
        "            for i in range(n_samples):\n",
        "                linear_output = np.dot(self.weights, X[i]) + self.bias\n",
        "                y_pred = self.step_function(linear_output)\n",
        "                # Update rule\n",
        "                update = self.learning_rate * (y[i] - y_pred)\n",
        "                self.weights += update * X[i]\n",
        "                self.bias += update\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        y_pred = self.step_function(linear_output)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# Input data (for all gates)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "\n",
        "#  AND Gate\n",
        "y_and = np.array([0, 0, 0, 1])\n",
        "and_gate = Perceptron(learning_rate=0.1, epochs=10)\n",
        "and_gate.fit(X, y_and)\n",
        "print(\"Predictions for AND Gate:\")\n",
        "for i in range(len(X)):\n",
        "    print(f\"Input: {X[i]} -> Output: {and_gate.predict(X[i])}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "#  OR Gate\n",
        "y_or = np.array([0, 1, 1, 1])\n",
        "or_gate = Perceptron(learning_rate=0.1, epochs=10)\n",
        "or_gate.fit(X, y_or)\n",
        "print(\"Predictions for OR Gate:\")\n",
        "for i in range(len(X)):\n",
        "    print(f\"Input: {X[i]} -> Output: {or_gate.predict(X[i])}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "#  XOR Gate\n",
        "y_xor = np.array([0, 1, 1, 0])\n",
        "xor_gate = Perceptron(learning_rate=0.1, epochs=10)\n",
        "xor_gate.fit(X, y_xor)\n",
        "print(\"Predictions for XOR Gate:\")\n",
        "for i in range(len(X)):\n",
        "    print(f\"Input: {X[i]} -> Output: {xor_gate.predict(X[i])}\")\n",
        "print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=0)\n",
        "\n",
        "perceptron = Perceptron(alpha=0.1)\n",
        "perceptron.fit(X_train, y_train)\n",
        "y_pred = perceptron.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "J_FT5luRXEcR",
        "outputId": "bcd9d5b9-0f6a-4572-c6fd-0b9d659f0f46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# MLP with Single Hidden Layer (Manual + sklearn)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# --- Activation Function ---\n",
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# --- Neuron Class ---\n",
        "class Neuron:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def feedforward(self, inputs):\n",
        "        \"\"\"Compute neuron output using weights, bias, and activation function.\"\"\"\n",
        "        total = np.dot(self.weights, inputs) + self.bias\n",
        "        return sigmoid(total)\n",
        "\n",
        "# Neural Network Class\n",
        "class OurNeuralNetwork:\n",
        "    \"\"\"\n",
        "    A neural network with:\n",
        "      - 2 inputs\n",
        "      - a hidden layer with 2 neurons (h1, h2)\n",
        "      - an output layer with 1 neuron (o1)\n",
        "    Each neuron has the same weights and bias:\n",
        "      - w = [0, 1]\n",
        "      - b = 0\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        weights = np.array([0, 1])\n",
        "        bias = 0\n",
        "\n",
        "        # Hidden layer neurons\n",
        "        self.h1 = Neuron(weights, bias)\n",
        "        self.h2 = Neuron(weights, bias)\n",
        "        # Output layer neuron\n",
        "        self.o1 = Neuron(weights, bias)\n",
        "\n",
        "    def feedforward(self, x):\n",
        "        out_h1 = self.h1.feedforward(x)\n",
        "        out_h2 = self.h2.feedforward(x)\n",
        "        # The inputs for o1 are the outputs from h1 and h2\n",
        "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
        "        return out_o1\n",
        "\n",
        "\n",
        "#   Test Our Simple Neural Network\n",
        "network = OurNeuralNetwork()\n",
        "x = np.array([2, 3])\n",
        "print(\"Output of simple neural network:\", network.feedforward(x))  # 0.7216...\n",
        "\n",
        "\n",
        "#   MNIST Dataset Using sklearn MLPClassifier\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Load MNIST dataset (70000 samples of 28x28 pixel digits)\n",
        "data, labels = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "print(\"Dataset shape:\", data.shape)\n",
        "\n",
        "# Normalize pixel values (0-255) to (0-1)\n",
        "data = data / 255.0\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data, labels, test_size=0.10, random_state=42, stratify=labels\n",
        ")\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape)\n",
        "\n",
        "# Create and train MLP model\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=50, verbose=1, random_state=1)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "print(\"Training set score:\", mlp.score(X_train, y_train))\n",
        "print(\"Testing set score:\", mlp.score(X_test, y_test))\n",
        "\n",
        "# Test a single prediction\n",
        "index = 346\n",
        "test_digit = X_test.iloc[index].to_numpy().reshape(1, 784)\n",
        "test_digit_prediction = mlp.predict(test_digit)[0]\n",
        "\n",
        "print(\"Predicted value:\", test_digit_prediction)\n",
        "print(\"Actual value:\", y_test.iloc[index])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJufgsSUVPWb",
        "outputId": "74839c0c-e279-4ebb-d868-1a0635d4e8a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of simple neural network: 0.7216325609518421\n",
            "Dataset shape: (70000, 784)\n",
            "Training set shape: (63000, 784)\n",
            "Testing set shape: (7000, 784)\n",
            "Iteration 1, loss = 0.50240991\n",
            "Iteration 2, loss = 0.24293823\n",
            "Iteration 3, loss = 0.19396647\n",
            "Iteration 4, loss = 0.16213879\n",
            "Iteration 5, loss = 0.14071148\n",
            "Iteration 6, loss = 0.12341883\n",
            "Iteration 7, loss = 0.10922166\n",
            "Iteration 8, loss = 0.09774355\n",
            "Iteration 9, loss = 0.08919639\n",
            "Iteration 10, loss = 0.08139225\n",
            "Iteration 11, loss = 0.07540877\n",
            "Iteration 12, loss = 0.06954664\n",
            "Iteration 13, loss = 0.06361726\n",
            "Iteration 14, loss = 0.05979103\n",
            "Iteration 15, loss = 0.05585580\n",
            "Iteration 16, loss = 0.05202626\n",
            "Iteration 17, loss = 0.04899432\n",
            "Iteration 18, loss = 0.04561763\n",
            "Iteration 19, loss = 0.04281492\n",
            "Iteration 20, loss = 0.04050308\n",
            "Iteration 21, loss = 0.03787670\n",
            "Iteration 22, loss = 0.03558628\n",
            "Iteration 23, loss = 0.03292704\n",
            "Iteration 24, loss = 0.03132445\n",
            "Iteration 25, loss = 0.02988745\n",
            "Iteration 26, loss = 0.02857582\n",
            "Iteration 27, loss = 0.02623057\n",
            "Iteration 28, loss = 0.02454969\n",
            "Iteration 29, loss = 0.02335991\n",
            "Iteration 30, loss = 0.02190347\n",
            "Iteration 31, loss = 0.02054652\n",
            "Iteration 32, loss = 0.01955444\n",
            "Iteration 33, loss = 0.01806062\n",
            "Iteration 34, loss = 0.01723213\n",
            "Iteration 35, loss = 0.01601015\n",
            "Iteration 36, loss = 0.01543006\n",
            "Iteration 37, loss = 0.01425774\n",
            "Iteration 38, loss = 0.01361097\n",
            "Iteration 39, loss = 0.01272899\n",
            "Iteration 40, loss = 0.01192553\n",
            "Iteration 41, loss = 0.01160828\n",
            "Iteration 42, loss = 0.01075935\n",
            "Iteration 43, loss = 0.00983147\n",
            "Iteration 44, loss = 0.00952984\n",
            "Iteration 45, loss = 0.00876814\n",
            "Iteration 46, loss = 0.00864342\n",
            "Iteration 47, loss = 0.00796637\n",
            "Iteration 48, loss = 0.00720160\n",
            "Iteration 49, loss = 0.00758951\n",
            "Iteration 50, loss = 0.00681590\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set score: 0.9985079365079365\n",
            "Testing set score: 0.97\n",
            "Predicted value: 9\n",
            "Actual value: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}